import os
from typing import List
from loguru import logger
import time

try:
    from langchain_community.embeddings import DashScopeEmbeddings
    from langchain_community.vectorstores import FAISS
    from langchain_core.documents import Document
except ImportError as e:
    logger.error(f"Import error: {e}")
    raise


class VectorStoreManager:
    def __init__(self, embeddings_model: str = "text-embedding-v1"):
        dashscope_api_key = os.getenv("DASHSCOPE_API_KEY")
        if not dashscope_api_key:
            raise ValueError("DASHSCOPE_API_KEY environment variable is not set")

        # ç§»é™¤ timeout å‚æ•°
        self.embeddings = DashScopeEmbeddings(
            model=embeddings_model,
            dashscope_api_key=dashscope_api_key
        )
        self.vector_store = None

    def create_vector_store(self, documents: List[Document]) -> None:
        """åˆ†æ‰¹åˆ›å»ºå‘é‡å­˜å‚¨ï¼Œé¿å…è¶…æ—¶"""
        logger.info(f"Creating vector store with {len(documents)} documents")

        # åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹50ä¸ªæ–‡æ¡£ï¼ˆæ›´å°çš„æ‰¹æ¬¡ï¼‰
        batch_size = 50
        total_batches = (len(documents) + batch_size - 1) // batch_size

        for batch_num in range(total_batches):
            start_idx = batch_num * batch_size
            end_idx = min((batch_num + 1) * batch_size, len(documents))
            batch = documents[start_idx:end_idx]

            print(f"ğŸ”§ å‘é‡åŒ–è¿›åº¦: {batch_num + 1}/{total_batches} ({len(batch)} ä¸ªæ–‡æ¡£)")

            try:
                if not self.vector_store:
                    # ç¬¬ä¸€æ‰¹åˆ›å»ºæ–°çš„å‘é‡å­˜å‚¨
                    self.vector_store = FAISS.from_documents(batch, self.embeddings)
                    print(f"   âœ… ç¬¬ä¸€æ‰¹å‘é‡åŒ–å®Œæˆ")
                else:
                    # åç»­æ‰¹æ¬¡æ·»åŠ åˆ°ç°æœ‰å­˜å‚¨
                    self.vector_store.add_documents(batch)
                    print(f"   âœ… ç¬¬{batch_num + 1}æ‰¹å‘é‡åŒ–å®Œæˆ")

                # æ‰¹æ¬¡é—´å»¶è¿Ÿï¼Œé¿å…APIé™åˆ¶
                if batch_num < total_batches - 1:  # ä¸æ˜¯æœ€åä¸€æ‰¹
                    time.sleep(3)  # 3ç§’å»¶è¿Ÿï¼Œç»™APIæ›´å¤šä¼‘æ¯æ—¶é—´

            except Exception as e:
                print(f"   âŒ ç¬¬{batch_num + 1}æ‰¹å¤„ç†å¤±è´¥: {e}")
                # ç»§ç»­å¤„ç†ä¸‹ä¸€æ‰¹ï¼Œä¸ä¸­æ–­
                continue

        print("âœ… å‘é‡å­˜å‚¨åˆ›å»ºå®Œæˆ")
        logger.info("Vector store created successfully")

    def save_vector_store(self, path: str) -> None:
        """ä¿å­˜å‘é‡å­˜å‚¨åˆ°ç£ç›˜"""
        if self.vector_store:
            self.vector_store.save_local(path)
            logger.info(f"Vector store saved to {path}")

    def load_vector_store(self, path: str) -> None:
        """ä»ç£ç›˜åŠ è½½å‘é‡å­˜å‚¨"""
        self.vector_store = FAISS.load_local(path, self.embeddings, allow_dangerous_deserialization=True)
        logger.info(f"Vector store loaded from {path}")

    def similarity_search(self, query: str, k: int = 4) -> List[Document]:
        """ç›¸ä¼¼åº¦æœç´¢"""
        if not self.vector_store:
            raise ValueError("Vector store not initialized")

        results = self.vector_store.similarity_search(query, k=k)
        logger.info(f"Found {len(results)} relevant documents for query: {query}")
        return results